{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "\n",
    "import requests\n",
    "import psycopg2\n",
    "from psycopg2 import connect, sql\n",
    "\n",
    "# Configure your PostgreSQL connection string\n",
    "conn_string = \"dbname='etl_bites' user='ilhaam.ahmed' password='etl_proj' host='localhost' port='5432'\"\n",
    "\n",
    "def get_data_from_api(url):\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "posts_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "users_url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "posts_data = get_data_from_api(posts_url)\n",
    "users_data = get_data_from_api(users_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is fetching data from two different API endpoints (posts and users) using the 'requests' library which I am lready familiar with. It is then storing the retrieved JSON data in the 'posts_data' and 'users_data' variables.\n",
    "\n",
    "Reminder:\n",
    "- 'requests' - sends HTTP requests to web servers and receives responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "\n",
    "def join_posts_and_users(posts, users):\n",
    "    for post in posts:\n",
    "        for user in users:\n",
    "            if post['userId'] == user['id']:\n",
    "                post['author'] = user['name']\n",
    "    return posts\n",
    "\n",
    "combined_data = join_posts_and_users(posts_data, users_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, a join_posts_and_users() function is defined which takes 'posts' and 'users' as parameters. Within the function, a simple join operation is made: so for each post, the function iterates over each element in the users list and checks for matching user and post IDs.\n",
    "\n",
    "Then the data is combined and stored in the 'combine_data' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "\n",
    "# Create tables in analytical DB\n",
    "# This could also be done manually via a GUI (e.g. TablePlus) or with a SQL script\n",
    "def execute_query_postgresql(conn_string, query):\n",
    "    with connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "\n",
    "# check if table exists before creating them\n",
    "def table_exists(conn_string, table_name):\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = '{table_name}')\")\n",
    "            return cur.fetchone()[0]\n",
    "\n",
    "# Create new tables\n",
    "\n",
    "if not table_exists(conn_string, \"api_data\"):\n",
    "    create_api_data_table = '''\n",
    "    CREATE TABLE api_data (\n",
    "        post_id INTEGER NOT NULL,\n",
    "        title TEXT NOT NULL,\n",
    "        body TEXT NOT NULL,\n",
    "        user_id INTEGER NOT NULL,\n",
    "        author TEXT NOT NULL\n",
    "    );\n",
    "    '''\n",
    "    execute_query_postgresql(conn_string, create_api_data_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part above, defined the execute_query_postgresql() function to connect to the postgres database and uses cursors to execute the sql queries as well as commit them to the database.\n",
    "\n",
    "Then an 'api_data' table is created with the specific columns.\n",
    "\n",
    "Finally the table created query is executed using the execute_query_postgresql() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the data - Load\n",
    "\n",
    "def insert_data_to_postgresql(conn_string, table_name, data):\n",
    "    with connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            for item in data:\n",
    "                query = sql.SQL(\"INSERT INTO {} (post_id, title, body, user_id, author) VALUES (%s, %s, %s, %s, %s)\").format(sql.Identifier(table_name))\n",
    "                cur.execute(query, (item['id'], item['title'], item['body'], item['userId'], item['author']))\n",
    "        conn.commit()\n",
    "\n",
    "table_name = \"api_data\"\n",
    "insert_data_to_postgresql(conn_string, table_name, combined_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last part defines the insert_data_to_postgresql() function, which is used to insert the fetched API data into a postgres database table called 'api_data'.\n",
    "\n",
    "'connect' is used to establish a connection, then a cursor is used to execute the sql queries.\n",
    "\n",
    "A breakdown of the 'query' variable:\n",
    "- The 'sql.SQL' function is used to format the query and prevent issues (malicious SQL statements being inserted).\n",
    "- The 'sql.Identifier(table_name)' part safely formats the table name as an identifier, which helps prevent SQL injection attacks on the table name.\n",
    "- The 'cur.execute' method executes the query and inserts the data into the table.\n",
    "- The changes are then commited."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "### Extract the todos and users data from the API, and calculate the number of completed tasks for each user.\n",
    "\n",
    "### Load the result into a new table in the local PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract \n",
    "\n",
    "todos_url = \"https://jsonplaceholder.typicode.com/todos\"\n",
    "todos_data = get_data_from_api(todos_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "\n",
    "def calculate_completed_tasks(todos, users):\n",
    "    completed_tasks = {}\n",
    "\n",
    "    for todo in todos:\n",
    "        user_id = todo['userId']\n",
    "        if todo['completed']:\n",
    "            completed_tasks[user_id] = completed_tasks.get(user_id, 0) + 1\n",
    "\n",
    "    result = []\n",
    "    for user in users:\n",
    "        result.append({'userId': user['id'], 'name': user['name'], 'completedTasks': completed_tasks[user['id']]})\n",
    "    return result\n",
    "\n",
    "user_completed_tasks = calculate_completed_tasks(todos_data, users_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "\n",
    "def execute_query_postgresql(conn_string, query):\n",
    "    with connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "\n",
    "# check if table exists before creating them\n",
    "def table_exists(conn_string, table_name):\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = '{table_name}')\")\n",
    "            return cur.fetchone()[0]\n",
    "\n",
    "# Create new tables\n",
    "\n",
    "if not table_exists(conn_string, \"user_completed_tasks\"):\n",
    "    create_user_completed_tasks_table = '''\n",
    "    CREATE TABLE user_completed_tasks (\n",
    "        user_id INTEGER NOT NULL,\n",
    "        name TEXT NOT NULL,\n",
    "        completed_tasks INTEGER NOT NULL\n",
    "    );\n",
    "    '''\n",
    "    execute_query_postgresql(conn_string, create_user_completed_tasks_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into table\n",
    "\n",
    "def insert_data_to_postgresql(conn_string, table_name, data):\n",
    "    with connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            for item in data:\n",
    "                query = sql.SQL(\"INSERT INTO {} (user_id, name, completed_tasks) VALUES (%s, %s, %s)\").format(sql.Identifier(table_name))\n",
    "                cur.execute(query, (item['userId'], item['name'], item['completedTasks']))\n",
    "        conn.commit()\n",
    "\n",
    "table_name = \"user_completed_tasks\"\n",
    "insert_data_to_postgresql(conn_string, table_name, user_completed_tasks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHALLENGE\n",
    "\n",
    "### Extract all repositories from the GitHub API for a specific user, and calculate the total number of stars they received. Use pagination when fetching the data from the API. Load the result into a new table in the local PostgreSQL database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytical_databases_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
