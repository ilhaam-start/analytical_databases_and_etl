{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import csv\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv_from_s3(bucket_name, object_key):\n",
    "    s3 = boto3.client('s3', config=botocore.config.Config(signature_version=botocore.UNSIGNED))\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Process header row to remove leading and trailing spaces\n",
    "    header, rows = content.split(\"\\n\", 1)\n",
    "    cleaned_header = \",\".join(column.strip() for column in header.split(\",\"))\n",
    "\n",
    "    # Return cleaned content\n",
    "    return cleaned_header + \"\\n\" + rows\n",
    "\n",
    "titanic_csv_content = download_csv_from_s3('data-eng-makers-public-datasets-404544469985', 'etl_bites_04_titanic_dataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the necessary libraries are imported, here is what each is for:\n",
    "- boto3 = This is for interacting with AWS services\n",
    "- botocore = This is for configuring the client\n",
    "- csv = This is for CSV processing\n",
    "- StringIO = This is for handling strings as file-like objects.\n",
    "\n",
    "Then the download_csv_from_s3() function is defined, whicn takes two parameters. Inside the function, the Boto3 S3 client is created and configured. The configuration 'signature_version=botocore.UNSIGNED' is used when accessing a public S3 bucket that doesn't require authentication.\n",
    "\n",
    "The get_object() method is called to retrieve the object from the specified bucket and key. The response content is read and decoded as UTF-8.\n",
    "\n",
    "The header row is processed to remove any leading and trailing spaces using a list comprehension. The cleaned content is then returned as a string.\n",
    "\n",
    "Lastly the download_csv_from_s3() function is called to extract the content of a specific CSV file named 'etl_bites_04_titanic_dataset.csv' from the S3 bucket 'data-eng-makers-public-datasets-404544469985'.\n",
    "\n",
    "Overall, what this code is doing is enabling the download of a CSV file from S3 bucket, makin it available for further processing or loading into a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_fare(titanic_data, pclass_filter):\n",
    "    total_fare = 0\n",
    "    passengers_count = 0\n",
    "\n",
    "    for row in titanic_data:\n",
    "        if row['Pclass'] == str(pclass_filter):\n",
    "            total_fare += float(row['Fare'])\n",
    "            passengers_count += 1\n",
    "\n",
    "    return total_fare / passengers_count if passengers_count > 0 else 0\n",
    "\n",
    "average_fare_class_1 = calculate_average_fare(csv.DictReader(StringIO(titanic_csv_content)), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the calculate_average_fare() function is defined and takes two parameters: titanic_data and pclass_filter (This is the passenger class: 1,2,or 3). The total_fare and passengers_count variables are both set to zero.\n",
    "\n",
    "Within the function we create a loop and iterate over each row in the titanic_data. We ensure that the passengers_count is increased by 1 if the item in the row in the column 'Pclass' matches the pclass_filter passed as an argument. We also ensure that the total_fare is increased by the amount in the 'Fare' column for that row.\n",
    "\n",
    "We then end the loop and return the total_fare divided by the passengers_count if the passengers_count is bigger than 0, otherwise we return 0.\n",
    "\n",
    "Finally we call the calculate_average_fare() function and convert the 'titanic_csv_content' dataset into a csv.DictReader object. The 'pclass_filter' is set to 1 to calculate the average fare for the first class.\n",
    "\n",
    "To summarise, the code calculates the average fare for a specific passenger class (1, 2, or 3) in the Titanic dataset. It iterates over the dataset, sums the fares for the passengers in the specified class, and divides the sum by the count of passengers to compute the average fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_class_average_fares_table = '''CREATE TABLE class_average_fares (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    pclass INTEGER NOT NULL,\n",
    "    average_fare NUMERIC(10, 2) NOT NULL\n",
    ");'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created the table in TablePlus instead, but storing the details above for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_postgresql(data, connection):\n",
    "    cursor = connection.cursor()\n",
    "    query = \"INSERT INTO class_average_fares (pclass, average_fare) VALUES (%s, %s)\"\n",
    "    cursor.execute(query, data)\n",
    "    connection.commit()\n",
    "\n",
    "etl_bites_conn_string = \"\"\"\n",
    "    host='localhost' \n",
    "    port='5432' \n",
    "    dbname='etl_bites' \n",
    "    user='ilhaam.ahmed' \n",
    "    password='etl_proj'\"\"\"\n",
    "conn = psycopg2.connect(etl_bites_conn_string)\n",
    "\n",
    "insert_data_to_postgresql((1, average_fare_class_1), conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the insert_data_to_postgresql() function is defined, this is used to insert and commit the data to the database. A connection is established with the database and the insert_data_to_postgresql() is called in order insert the data to the database. It takes '(1, average_fare_class_1), conn' as values. The 1 is the passenger class and the average_fare_class_1 is th amount we previously calculated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "### Calculate the survival rate for passengers in the 2nd class from the Titanic dataset.\n",
    "\n",
    "### Create a new table class_survival_rate in the local PostgreSQL database and insert the calculated survival rate into this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the survival rate for the given passenger class.\n",
    "\n",
    "def calculate_survival_rate(csv_content, pclass):\n",
    "    csv_reader = csv.DictReader(csv_content.splitlines())\n",
    "    count = 0\n",
    "    survivors = 0\n",
    "\n",
    "    for row in csv_reader:\n",
    "        if int(row['Pclass']) == pclass:\n",
    "            count += 1\n",
    "            if int(row['Survived']) == 1:\n",
    "                survivors += 1\n",
    "\n",
    "    return survivors / count if count > 0 else None\n",
    "\n",
    "# Then, you can use it to calculate the survival rate for the 2nd class:\n",
    "\n",
    "titanic_csv = download_csv_from_s3('data-eng-makers-public-datasets-404544469985', 'etl_bites_04_titanic_dataset.csv')\n",
    "survival_rate_class_2 = calculate_survival_rate(titanic_csv, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = '''\n",
    "CREATE TABLE class_survival_rate (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    pclass INTEGER NOT NULL,\n",
    "    survival_rate NUMERIC(10, 5) NOT NULL\n",
    ");'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_survival_rate_to_postgresql(survival_rate_data, connection):\n",
    "    with connection.cursor() as cursor:\n",
    "        query = \"INSERT INTO class_survival_rate (pclass, survival_rate) VALUES (%s, %s)\"\n",
    "        cursor.execute(query, survival_rate_data)\n",
    "    connection.commit()\n",
    "\n",
    "insert_survival_rate_to_postgresql((2, survival_rate_class_2), conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "### For the challenge, we will use a different dataset, the Iris Species one.\n",
    "\n",
    "### Your challenge is to find the average sepal length and sepal width for each species and store the results in a local PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv_from_s3(bucket_name, object_key):\n",
    "    s3 = boto3.client('s3', config=botocore.config.Config(signature_version=botocore.UNSIGNED))\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Process header row to remove leading and trailing spaces\n",
    "    header, rows = content.split(\"\\n\", 1)\n",
    "    cleaned_header = \",\".join(column.strip() for column in header.split(\",\"))\n",
    "\n",
    "    # Return cleaned content\n",
    "    return cleaned_header + \"\\n\" + rows\n",
    "\n",
    "iris_species_csv_content = download_csv_from_s3('data-eng-makers-public-datasets-404544469985', 'etl_bites_04_iris_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = \"\"\"\n",
    "    host='localhost' \n",
    "    port='5432' \n",
    "    dbname='etl_bites' \n",
    "    user='ilhaam.ahmed' \n",
    "    password='etl_proj'\n",
    "\"\"\"\n",
    "\n",
    "def insert_iris_data_to_postgresql(conn_string, table_name, data):\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            for item in data.intertuples(index=False):\n",
    "                query = \"INSERT INTO {table_name} (species, sepal_length, sepal_width) VALUES (%s, %s, %s)\"\n",
    "                cur.execute(query, item)\n",
    "            conn.commit()\n",
    "\n",
    "def calculate_average_sepal_measurements(csv_content):\n",
    "    data = csv_content.split(\"\\n\")[1:]\n",
    "    species_measurements = {}\n",
    "\n",
    "    for row in data:\n",
    "        if not row:\n",
    "            continue\n",
    "\n",
    "        sepal_length, sepal_width, species = map(str.strip, row.split(\",\"))\n",
    "        sepal_length = float(sepal_length)\n",
    "        sepal_width = float(sepal_width)\n",
    "\n",
    "        measurements = species_measurements.setdefault(species, {'sepal_length_sum': 0, 'sepal_width_sum':0, 'count':0})\n",
    "        measurements['sepal_length_sum'] += sepal_length\n",
    "        measurements['sepal_width_sum'] += sepal_width\n",
    "        measurements['count'] += 1\n",
    "    \n",
    "    result = [(species, measurements['sepal_length_sum'] / measurements['count'], measurements['sepal_width_sum'] / measurements['count']) for species, measurements in species_measurements.items()]\n",
    "\n",
    "    return result\n",
    "\n",
    "iris_species_csv_content = download_csv_from_s3('data-eng-makers-public-datasets-404544469985', 'etl_bites_04_iris_dataset.csv')\n",
    "\n",
    "average_measurements = calculate_average_sepal_measurements(iris_species_csv_content)\n",
    "\n",
    "table_name = 'iris_data'\n",
    "insert_iris_data_to_postgresql(conn_string, table_name, average_measurements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytical_databases_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
